# backend/IA/transcriptiondiarization.py
import os
import subprocess
from dotenv import load_dotenv
from groq import Groq
from pyannote.audio import Pipeline

# 1Ô∏è‚É£ Charger les variables d'environnement
load_dotenv()
print("‚úÖ Cl√©s API charg√©es correctement.")

# 2Ô∏è‚É£ V√©rifier les cl√©s API
groq_api_key = os.getenv("GROQ_API_KEY")
hf_token = os.getenv("HUGGINGFACE_TOKEN")

if not groq_api_key or not hf_token:
    raise ValueError("‚ùå Cl√©s API manquantes (Groq ou Hugging Face)")

# 3Ô∏è‚É£ Configuration de base
base_dir = os.path.dirname(__file__)
audio_path = os.path.join(base_dir, "audio", "meet1.mp3")

# 4Ô∏è‚É£ Conversion en WAV
def convert_to_wav(audio_path):
    base, ext = os.path.splitext(audio_path)
    wav_path = base + ".wav"
    if not os.path.exists(wav_path):
        print(f"üéß Conversion du fichier {audio_path} en {wav_path} ...")
        subprocess.run([
            "ffmpeg", "-y",
            "-i", audio_path,
            "-ar", "16000",
            "-ac", "1",
            wav_path
        ], check=True)
    return wav_path

wav_path = convert_to_wav(audio_path)
print(f"‚úÖ Fichier WAV pr√™t : {wav_path}")

# 5Ô∏è‚É£ Charger le pipeline Pyannote    
print("‚è≥ Chargement du pipeline de diarisation (pyannote)...")
pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization", use_auth_token=hf_token)
print("‚úÖ Pipeline charg√© avec succ√®s !")

# 6Ô∏è‚É£ Fonction utilitaire pour formater le temps en mm:ss.s
def format_time(seconds):
    minutes = int(seconds // 60)
    secs = seconds % 60
    return f"{minutes:02d}:{secs:04.1f}"

# 7Ô∏è‚É£ Fusion diarisation + transcription avec timestamps
def match_speaker_to_text(diar_segments, text_segments):
    result = []
    for txt in text_segments:
        start = txt["start"]
        end = txt["end"]
        text = txt["text"].strip()

        # Trouver le locuteur le plus probable
        speaker = "UNKNOWN"
        for d in diar_segments:
            if d["start"] <= start <= d["end"]:
                speaker = d["speaker"]
                break

        # Ajout du timestamp format√©
        start_f = format_time(start)
        end_f = format_time(end)
        result.append(f"[{start_f} - {end_f}] [{speaker}] {text}")
    return result

# 8Ô∏è‚É£ Fonction principale
def transcription_with_diarization(audio_file=None):
    """
    Retourne le texte complet avec diarisation et timestamps.
    Si audio_file est fourni, on l'utilise √† la place du fichier par d√©faut.
    """
    global audio_path, wav_path

    if audio_file:
        audio_path = audio_file
        wav_path = convert_to_wav(audio_path)

    # Diarisation
    print("üéß D√©tection des intervenants...")
    diarization = pipeline(wav_path)
    segments = [{"start": t.start, "end": t.end, "speaker": s} for t, _, s in diarization.itertracks(yield_label=True)]
    print(f"üë• Intervenants d√©tect√©s : {set(seg['speaker'] for seg in segments)}")

    # Transcription Groq
    print("\nüéôÔ∏è Lancement de la transcription compl√®te (Groq)...")
    client = Groq(api_key=groq_api_key)
    with open(wav_path, "rb") as file:
        transcription = client.audio.transcriptions.create(
            file=file,
            model="whisper-large-v3-turbo",
            response_format="verbose_json",
            timestamp_granularities=["segment"],
            language="fr"
        )

    fusion = match_speaker_to_text(segments, transcription.segments)

    # Sauvegarde dans un fichier texte
    output_path = os.path.join(base_dir, "transcription_avec_diarisation.txt")
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(fusion))
    print(f"\n‚úÖ Transcription avec timestamps enregistr√©e ici : {output_path}\n")

    # Retour du texte fusionn√©
    return "\n".join(fusion)
