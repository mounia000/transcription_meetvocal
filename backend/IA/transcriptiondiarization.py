# backend/IA/transcriptiondiarization.py
import os
import subprocess
from dotenv import load_dotenv
from groq import Groq
from pyannote.audio import Pipeline

# 1Ô∏è‚É£ Charger les variables d'environnement
load_dotenv()
print("‚úÖ Cl√©s API charg√©es correctement.")

# 2Ô∏è‚É£ V√©rifier les cl√©s API
groq_api_key = os.getenv("GROQ_API_KEY")
hf_token = os.getenv("HUGGINGFACE_TOKEN")

if not groq_api_key or not hf_token:
    raise ValueError("‚ùå Cl√©s API manquantes (Groq ou Hugging Face)")

# 3Ô∏è‚É£ R√©pertoire audio
base_dir = os.path.dirname(__file__)
audio_dir = os.path.join(base_dir, "audio")
os.makedirs(audio_dir, exist_ok=True)

# 4Ô∏è‚É£ Conversion en WAV
def convert_to_wav(audio_path: str) -> str:
    """Convertit un fichier audio en WAV mono 16kHz."""
    base, ext = os.path.splitext(audio_path)
    wav_path = base + ".wav"
    if not os.path.exists(wav_path):
        print(f"üéß Conversion du fichier {audio_path} en {wav_path} ...")
        subprocess.run([
            "ffmpeg", "-y",
            "-i", audio_path,
            "-ar", "16000",
            "-ac", "1",
            wav_path
        ], check=True)
    return wav_path

# 5Ô∏è‚É£ Chargement du pipeline Pyannote (une seule fois)
print("‚è≥ Chargement du pipeline de diarisation (pyannote)...")
pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization", use_auth_token=hf_token)
print("‚úÖ Pipeline charg√© avec succ√®s !")

# 6Ô∏è‚É£ Formatage du temps
def format_time(seconds: float) -> str:
    minutes = int(seconds // 60)
    secs = seconds % 60
    return f"{minutes:02d}:{secs:04.1f}"

# 7Ô∏è‚É£ Fusion diarisation + transcription
def match_speaker_to_text(diar_segments, text_segments):
    result = []
    for txt in text_segments:
        start = txt["start"]
        end = txt["end"]
        text = txt["text"].strip()

        speaker = "UNKNOWN"
        for d in diar_segments:
            if d["start"] <= start <= d["end"]:
                speaker = d["speaker"]
                break

        result.append(f"[{format_time(start)} - {format_time(end)}] [{speaker}] {text}")
    return result

# 8Ô∏è‚É£ Fonction principale : ex√©cution sur un fichier upload√©
def transcription_with_diarization(audio_file: str):
    """
    Ex√©cute la transcription + diarisation sur un fichier audio upload√©.
    - audio_file : chemin complet du fichier audio (mp3, wav, etc.)
    """
    if not os.path.exists(audio_file):
        raise FileNotFoundError(f"‚ùå Fichier introuvable : {audio_file}")

    # Conversion en WAV
    wav_path = convert_to_wav(audio_file)

    # √âtape 1 : Diarisation
    print("üéß D√©tection des intervenants...")
    diarization = pipeline(wav_path)
    diar_segments = [{"start": t.start, "end": t.end, "speaker": s}
                     for t, _, s in diarization.itertracks(yield_label=True)]
    print(f"üë• Intervenants d√©tect√©s : {set(d['speaker'] for d in diar_segments)}")

    # √âtape 2 : Transcription (Groq)
    print("üéôÔ∏è Lancement de la transcription compl√®te (Groq)...")
    client = Groq(api_key=groq_api_key)
    with open(wav_path, "rb") as f:
        transcription = client.audio.transcriptions.create(
            file=f,
            model="whisper-large-v3-turbo",
            response_format="verbose_json",
            timestamp_granularities=["segment"],
            language="fr"
        )

    # √âtape 3 : Fusion
    fusion = match_speaker_to_text(diar_segments, transcription.segments)

    # √âtape 4 : Sauvegarde du r√©sultat
    output_path = os.path.join(audio_dir, "transcription_avec_diarisation.txt")
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(fusion))
    print(f"‚úÖ Transcription enregistr√©e : {output_path}")

    return "\n".join(fusion)
