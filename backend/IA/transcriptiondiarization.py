# backend/IA/transcriptiondiarization.py
import os
import subprocess
from dotenv import load_dotenv
from groq import Groq
from pyannote.audio import Pipeline

# D√©claration du pipeline en global
pipeline = None

# 1Ô∏è‚É£ Charger les variables d'environnement
load_dotenv()
print("‚úÖ Cl√©s API charg√©es correctement.")

# 2Ô∏è‚É£ V√©rifier les cl√©s API
groq_api_key = os.getenv("GROQ_API_KEY")
hf_token = os.getenv("HUGGINGFACE_TOKEN")

if not groq_api_key or not hf_token:
    raise ValueError("‚ùå Cl√©s API manquantes (Groq ou Hugging Face)")

# 3Ô∏è‚É£ Charger le pipeline Pyannote (global)
try:
    print("‚è≥ Chargement du pipeline de diarisation (pyannote)...")
    pipeline = Pipeline.from_pretrained("pyannote/speaker-diarization", use_auth_token=hf_token)
    print("‚úÖ Pipeline charg√© avec succ√®s !")
except Exception as e:
    print(f"‚ö†Ô∏è Erreur lors du chargement du pipeline Pyannote : {e}")
    pipeline = None

# 4Ô∏è‚É£ Conversion en WAV
def convert_to_wav(audio_path):
    base, ext = os.path.splitext(audio_path)
    wav_path = base + ".wav"
    if not os.path.exists(wav_path):
        print(f"üéß Conversion du fichier {audio_path} en {wav_path} ...")
        subprocess.run([
            "ffmpeg", "-y",
            "-i", audio_path,
            "-ar", "16000",
            "-ac", "1",
            wav_path
        ], check=True)
    return wav_path

# 5Ô∏è‚É£ Formater le temps
def format_time(seconds):
    minutes = int(seconds // 60)
    secs = seconds % 60
    return f"{minutes:02d}:{secs:04.1f}"

# 6Ô∏è‚É£ Associer les segments texte ‚Üî locuteurs
def match_speaker_to_text(diar_segments, text_segments):
    result = []
    detected_speakers = sorted(set(d["speaker"] for d in diar_segments))
    for txt in text_segments:
        start, end, text = txt["start"], txt["end"], txt["text"].strip()
        speaker = None
        min_distance = float('inf')
        for d in diar_segments:
            if d["start"] <= start <= d["end"] or d["start"] <= end <= d["end"]:
                speaker = d["speaker"]
                break
            distance = min(abs(d["start"] - start), abs(d["end"] - end))
            if distance < min_distance:
                min_distance = distance
                speaker = d["speaker"]
        if not speaker:
            speaker = detected_speakers[0] if detected_speakers else "SPEAKER_00"
        start_f, end_f = format_time(start), format_time(end)
        result.append(f"[{start_f} - {end_f}] [{speaker}] {text}")
    return result

# 7Ô∏è‚É£ Transcription + Diarisation principale
def transcription_with_diarization(audio_file=None):
    global pipeline  # <<----- üî• IMPORTANT

    if not pipeline:
        raise RuntimeError("‚ùå Le pipeline Pyannote n'est pas initialis√©.")

    base_dir = os.path.dirname(__file__)
    if audio_file:
        audio_path = audio_file
    else:
        audio_path = os.path.join(base_dir, "audio", "reunion_projet_meetrecap.m4a")

    wav_path = convert_to_wav(audio_path)

    # üîπ √âtape 1 : Diarisation
    print("üéß D√©tection des intervenants...")
    diarization = pipeline(wav_path)
    segments = [
        {"start": t.start, "end": t.end, "speaker": s}
        for t, _, s in diarization.itertracks(yield_label=True)
    ]
    print(f"üë• Intervenants d√©tect√©s : {set(seg['speaker'] for seg in segments)}")

    # üîπ √âtape 2 : Transcription
    print("\nüéôÔ∏è Transcription compl√®te (Groq)...")
    client = Groq(api_key=groq_api_key)
    with open(wav_path, "rb") as file:
        transcription = client.audio.transcriptions.create(
            file=file,
            model="whisper-large-v3-turbo",
            response_format="verbose_json",
            timestamp_granularities=["segment"],
            language="fr"
        )

    # üîπ √âtape 3 : Fusion
    fusion = match_speaker_to_text(segments, transcription.segments)

    output_path = os.path.join(base_dir, "transcription_avec_diarisation.txt")
    with open(output_path, "w", encoding="utf-8") as f:
        f.write("\n".join(fusion))

    print(f"\n‚úÖ Transcription enregistr√©e ici : {output_path}")
    return "\n".join(fusion)
